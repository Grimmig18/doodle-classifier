{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dokumentation\n",
    "\n",
    "1. Problemstellung<br>\n",
    "2. Datenset<br>\n",
    "3. Datenaufbereitung<br>\n",
    "4. Algorithmen & Neuronale Netze<br>\n",
    "5. Vorstellung der Ergebnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemstellung:\n",
    "\n",
    "Maschinelles Lernen kann in verschiedenen Bereichen eingesetzt werden. Beispiele dafür sind die Vorhersage zukünftiger Ereignisse auf Basis von vorherigen Ereignissen oder auch die Mustererkennung.\n",
    "Mit der voranschreitenden Digitalisierung übernimmt Software immer mehr alltägliche Aufgaben. Ein sehr aktuelles Thema ist dabei das autonome Fahren, indem maschinelles Lernen eingesetzt wird.\n",
    "Ein weiteres Beispiel für den Einsatz von Erkennungssoftware ist die Gesichtserkennung, wie es viele Apple Nutzer in ihren aktuellen Smartphone wieder finden.\n",
    "Wie diese beiden Beispiele zeigen, wird der Bilderkunngen im Zusammenhang mit maschinellem Lernens eine hohe Bedeutung beigemessen.\n",
    "Deshalb haben wir uns dafür entschieden den Bereich von Bild- und Mustererkennung im Rahmen der Vorlesung genauer zu beleuchten.\n",
    "\n",
    "Um ein geeignetes Projekt für die Bilderkennung durchzuführen, mussten wir einige Probleme lösen und uns geeignetes Wissen in den Bereichen aneignen.\n",
    "\n",
    "Die zu klärenden Fragen sind unter anderem:\n",
    "- Welches Datenset eignet sich, um das Projekt erfolgreich abschließen zu können?\n",
    "- Wie müssen die Daten aufbereitet werden?\n",
    "- Wie \"sehen\" Maschinen?\n",
    "- Wie werden Muster erkannt?\n",
    "- Welche Algorithmen gibt es?\n",
    "- Worauf muss beim Lernen geachtet werden?\n",
    "- welche Parameter können variiert werden?\n",
    "- Wie wirkt sich die Variation einzelner Parameter auf die Ergebnisse aus?\n",
    "\n",
    "In dieser Dokumentation geben wir Antworten auf die Fragen, die wir uns gestellt haben und zeigen, wie unser Projekt durchgeführt wurde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenset\n",
    "\n",
    "Für die Auswahl eines geeigneten Datensets haben wir uns auf Datensets mit einer geringen visuellen Komplexität beschränkt. \n",
    "Konkret haben wir sämtliche Fotoaufnahmen ausgeschlossen, da wir zunächst mit simplen Daten erste Erfahrungen und Ergebnisse sammelen wollten.\n",
    "Des Weiteren wollten wir keine plakative Texterkennung erstellen, sondern an handgemalten Zeichnungen versuchen.\n",
    "\n",
    "Bei unsere Suche nach einem geeigneten Datenset haben wir diverse Datenquellen gefunden: \n",
    "\n",
    "1) [https://archive.ics.uci.edu/ml/datasets.php](https://archive.ics.uci.edu/ml/datasets.php)<br>\n",
    "2) [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)<br>\n",
    "3) [https://quickdraw.withgoogle.com/#](https://quickdraw.withgoogle.com/#)<br>\n",
    "\n",
    "Die ersten beiden Quellen schließen wir aufgrund der oben gegebenen Begründung für unser Projekt aus, weswegen die Entscheidung auf das von Google bereitgestellte Datenset fiel.\n",
    "\n",
    "Das ausgewählte Datenset hat zudem den Vorteil, dass die Zeichnungen lediglich in schwarz weiß vorliegen, was wiederum die Komplexität der Daten minimiert, da die Farbquantizierung den Rahmen der Arbeit und unseren Erfahrungen sprengen würde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenaufbereitung\n",
    "## Datenverarbeitung\n",
    "Die Daten, die in diesem Projekt verwendet werden, kommen ausschließlich aus dem [Google Quickdraw Dataset](https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/raw;tab=objects?prefix=&forceOnObjectsSortingFiltering=false).\n",
    "Um die ersten Schritte der Verarbeitung zu vereinfachen, stellt Google [in einem Repository](https://github.com/googlecreativelab/quickdraw-dataset) einige Informationen zur Beschaffenheit der Daten bereit.\n",
    "Zur Veranschaulichung soll hier beispielhaft das Datenset der `pizza.ndjson` betrachtet werden.\n",
    "\n",
    "### Die rohen Daten\n",
    "Die Datei mit den Rohdaten enthält zeilenweise Informationen über einzelne, von Menschen gezeichneten, Doodles.\n",
    "\n",
    "\n",
    "Eine Zeile enthält dabei Folgende Informationen:\n",
    "\n",
    "| Key          | Type                   | Description                                  |\n",
    "| ------------ | -----------------------| -------------------------------------------- |\n",
    "| key_id       | 64-bit unsigned integer| A unique identifier across all drawings.     |\n",
    "| word         | string                 | Category the player was prompted to draw.    |\n",
    "| recognized   | boolean                | Whether the word was recognized by the game. |\n",
    "| timestamp    | datetime               | When the drawing was created.                |\n",
    "| countrycode  | string                 | A two letter country code ([ISO 3166-1 alpha-2](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2)) of where the player was located. |\n",
    "| drawing      | string                 | A JSON array representing the vector drawing | \n",
    "\n",
    "Ein Beispiel aus `pizza.ndjson`:\n",
    "```json\n",
    "{\"word\":\"pizza\",\"countrycode\":\"SA\",\"timestamp\":\"2017-03-22 16:54:21.3692 UTC\",\"recognized\":true,\"key_id\":\"6595531419680768\",\"drawing\":[[[203,193,177,143,104,66,34,17,12,14,30,53,75,100,125,152,177,204,228,245,255,258,259,258,251,239,228,211,199,190,180,171,163,163],[51,45,45,52,74,106,148,192,233,264,288,305,313,315,315,303,278,244,207,174,143,117,92,75,61,48,40,35,33,33,33,34,42,42],[0,27,45,58,78,94,111,127,143,161,177,194,210,228,244,261,277,294,310,328,344,361,380,397,413,429,449,463,481,496,512,529,546,566]],[[139,137,137,134,126,116,109,109,111,118,121,123,124,123],[56,61,73,97,133,179,226,266,292,312,325,334,340,338],[965,994,1011,1027,1043,1061,1081,1098,1115,1130,1148,1166,1194,1232]],[[21,23,31,45,67,95,128,161,191,220,252,273,294,306,306],[199,206,210,212,212,204,190,179,173,167,157,151,146,144,144],[1399,1443,1462,1477,1494,1510,1527,1543,1561,1580,1599,1614,1628,1642,1663]],[[216,205,188,158,125,96,75,63,58,56,55,55,55,56],[60,71,93,128,175,222,266,299,320,337,348,357,364,363],[2049,2093,2110,2126,2143,2162,2177,2195,2220,2230,2247,2261,2277,2362]],[[53,60,69,85,108,134,162,188,207,223,234,243,243],[136,143,151,163,179,195,212,227,238,250,259,268,268],[2535,2561,2577,2594,2611,2630,2644,2660,2676,2699,2715,2731,2749]]]}\n",
    "```\n",
    "An dieser Stelle können bereits die Informationen gefilter werden, die für unser Projekt relevant sind.\n",
    "Zum einen ist das `\"word\": \"pizza\"` relevant, da es bei einer Klassifizierung die Ground Truth und somit das gewollte Ergebnis darstellt.\n",
    "Weiterhin ist das Feld `\"recognized\": true` für uns wichtig, da wird zuerst nur mit Zeichnungen arbeiten wollen, die auch von Google korrekt identifiziert worden sind.\n",
    "Ist `\"recognized\": false`, verwerfen wir das Datum.\n",
    "Zum wichtigsten Feld des Datensatz zählt das `\"drawing\"`, also die Zeichnung selbst.\n",
    "Eine Zeichnung ist in Form von einzelnen Strokes, also \"Pinselstrichen\", dargestellt.\n",
    "Ein Stroke wird durch `x, y` Koordinaten und einer Zeit `t` dargestellt und hat die From\n",
    "```json\n",
    "\"drawing\": [ \n",
    "  [  // First stroke \n",
    "    [x0, x1, x2, x3, ...],\n",
    "    [y0, y1, y2, y3, ...],\n",
    "    [t0, t1, t2, t3, ...]\n",
    "  ],\n",
    "  [  // Second stroke\n",
    "    [x0, x1, x2, x3, ...],\n",
    "    [y0, y1, y2, y3, ...],\n",
    "    [t0, t1, t2, t3, ...]\n",
    "  ],\n",
    "  ... // Additional strokes\n",
    "]\n",
    "```\n",
    "Final wollen wir als Resultat unserer Datenverabreitung ein `256x256` Bild erhalten.\n",
    "Dafür müssen aus den einzelnen Strokes die Pixel extrahiert werden, die von einem Stroke eingefärbt werden.\n",
    "Da wir nur mit schwarz-weiß Bildern hantieren, können wir das Bild in einer `256x256` Matrix bestehend aus `0` und `1` darstellen.\n",
    "Zunächst müssen aber die Strokes naher betrachtet werden.\n",
    "Zunächst kann die zeitliche Dimension verworfen werden, da wir uns nicht dafür interessieren wann und wie schnell ein Stroke gezeichnet wurde.\n",
    "Nun betrachten wir die `x, y` Koordinaten der Strokes.\n",
    "Zunächst gehen wir das Plotting naiv an.\n",
    "Betrachten wir die Strokes eines Bilds.\n",
    "Von den Strokes axtrahieren wir die maximale `x`- bzw. `y`- Koordinate mit\n",
    "```python\n",
    "import numnpy as np\n",
    "max_val = 0\n",
    "for stroke in img:\n",
    "  print(stroke)\n",
    "  for i in range(len(stroke[0])):\n",
    "    print(stroke[0][i], stroke[1][i])\n",
    "    if stroke[0][i] > max_val:\n",
    "      max_val = stroke[0][i]\n",
    "    if stroke[1][i] > max_val:\n",
    "      max_val = stroke[1][i]\n",
    "\n",
    "print(max_val)\n",
    "```\n",
    "wobei `img` das Array des `\"drawing\"`-Felds der Rohdaten enthält.\n",
    "Mit dieser Information erstellen wur nun eine `np.full((max_val, max_val), 1)` Matrix.\n",
    "Für jeden Punkt, der von einem Stroke bemalt wird, setzen wir den enstrepchenden Wert der Matrix auf `0`.\n",
    "```python\n",
    "for stroke in img:\n",
    "  for i in range(len(stroke[0])):\n",
    "    mat[stroke[0][i] - 1, stroke[1][i] - 1] = 0\n",
    "```\n",
    "Um die Matrix graphisch darzustellen, können wir [`PyPng`](https://pypng.readthedocs.io/en/latest/ex.html#a-palette) verwenden:\n",
    "\n",
    "```python\n",
    "import png\n",
    "mat = [[int(c) for c in row] for row in mat]\n",
    "w = png.Writer(len(mat[0]), len(mat), greyscale=True, bitdepth=1)\n",
    "f = open('image.png', 'wb')\n",
    "w.write(f, mat)\n",
    "f.close()\n",
    "```\n",
    "Das resultierende Bild sieht dann so aus:\n",
    "\n",
    "![pizza](../img/pizza_example_bad.png)\n",
    "\n",
    "Hier sehen bereits eines der Probleme: Die Strokes markieren nicht alle Punkte die tatsächlich eingefärbt werden müssen, sondern nur eine Teilmenge davon.\n",
    "Um die Bilder also richtig anzeigen und die Daten richtig verarbeiten zu können, müssen wir also die Pixel zwischen den einzelnen Punkten füllen.\n",
    "Dafür können wir die [Python Imaging Library](https://pillow.readthedocs.io/en/stable/) verwenden.\n",
    "```python\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "image = Image.new(\"RGB\", (max, max), color=(255,255,255))\n",
    "image_draw = ImageDraw.Draw(image)\n",
    "\n",
    "for stroke in img:\n",
    "  stroke_tup = [(stroke[0][i], stroke[1][i]) for i in range(len(stroke[0]))]\n",
    "  image_draw.line(stroke_tup, fill=(0,0,0), width=2)\n",
    "\n",
    "img_data = np.reshape(list(image.getdata()), (max_val, max_val, 3))\n",
    "\n",
    "\n",
    "mat = np.full((max_val, max_val), 1)\n",
    "\n",
    "for i, row in enumerate(img_data):\n",
    "  for j, val in enumerate(row):\n",
    "    if not np.array([v == 255 for v in val]).all():\n",
    "      mat[i,j] = 0\n",
    "\n",
    "mat = [[int(c) for c in row] for row in mat]\n",
    "w = png.Writer(len(mat[0]), len(mat), greyscale=True, bitdepth=1)\n",
    "f = open('better_pizza.png', 'wb')\n",
    "w.write(f, mat)\n",
    "f.close()\n",
    "```\n",
    "\n",
    "![pizza_better](../img/pizza_example_better.png)\n",
    "\n",
    "Jetzt sieht das schon viel besser aus\n",
    "\n",
    "Das letzte Problem, welches uns jetzt noch im Wege steht, ist die Auflösung der Bilder.\n",
    "Für unsere Verarbeitung wollen wir, dass alle Bilder eine Auflösung von `256x256` Pixeln haben.\n",
    "Dafür skalieren wir die Bilder in den meisten Fällen runter.\n",
    "Im Rahmen unserer Verarbeitung setzen wir direkt bei den Strokes an und berechnen diese neu, sodass alle `x, y` Werte zwischen 0 und 255 liegen:\n",
    "\n",
    "```python\n",
    "max_x, _, max_y, _, a = get_rescale_factors(strokes)\n",
    "\n",
    "for stroke in strokes:\n",
    "  stroke[0] = [x for x in np.rint(np.interp(stroke[0], (max_x - a, max_x), (0, 255))).tolist()]\n",
    "  stroke[1] = [y for y in np.rint(np.interp(stroke[1], (max_y - a, max_y), (0, 255))).tolist()]\n",
    "```\n",
    "\n",
    "`get_rescale_factors` ist dabei eine Funktion, die uns die maximale `x, y` Koordinaten zurürckgibt, die für die Skalierung benötigt werden.\n",
    "Um die Bilder auf das Nötigste zu begrenzen geben wir auch `a` zurück, welches die Seitenlänge eines Quadrats ist, die den relevanten Teil der Zeichnung perfekt eingrenzt.\n",
    "\n",
    "![pizza_done](../img/pizza_example_done.png)\n",
    "\n",
    "Die dargestellten Schritte führen wir für alle von Google erkannten Bilder aus und speichern die neu skalierten und gefüllte Strokes in einer neuen Datei in der Form:\n",
    "```json\n",
    "{\n",
    "    \"word\": \"pizza\", \n",
    "    \"drawing\": [[19, 19, 20, ...], [40, 41, 42, ...], ...]\n",
    "}\n",
    "```\n",
    "wobei das Feld `\"drawing\"` auch hier wieder die Strokes in (fast) gleicher Form wie die Rohdaten darstellt:\n",
    "```json\n",
    "\"drawing\": [ \n",
    "  [  // First stroke \n",
    "    [x0, x1, x2, x3, ...],\n",
    "    [y0, y1, y2, y3, ...]\n",
    "  ],\n",
    "  [  // Second stroke\n",
    "    [x0, x1, x2, x3, ...],\n",
    "    [y0, y1, y2, y3, ...]\n",
    "  ],\n",
    "  ... // Additional strokes\n",
    "]\n",
    "```\n",
    "Beachte, dass die zeitliche Dimension entfallen ist.\n",
    "Nach diesem letzten Schritt haben wir die Daten soweit aufgearbeitet, dass wir sie an unser CNN füttern können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmen & Neuronale Netze\n",
    "Um einen Überblick zu bekommen, wie die Erkennung von Bildern funktioniert, haben wir Informationen zu \"Image Classification\" gesucht.\n",
    "Dabei tauchen immer wieder die Begriffe \"Deep learning\" und \"Neuronalenetzwerke\" auf.\n",
    "Damit die Klassifizierung von Bildern mit Deep Learning erfolgreich ist, wird empfohlen neuronale Netze zu verwenden, die die Eingabe filtern und durch verschiedenen Datenlayer schicken. (Vgl. [Thinkautomation](https://www.thinkautomation.com/eli5/eli5-what-is-image-classification-in-deep-learning/))\n",
    "\n",
    "Es gibt eine Vielzahl unterschiedlicher Neuronaler Netze wie die folgende Liste zeigt (Vgl. [towards datascience](https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464), [towards datascience 2](https://towardsdatascience.com/types-of-neural-network-and-what-each-one-does-explained-d9b4c0ed63a1)):\n",
    "\n",
    "- Perceptron (P)\n",
    "- Feed Forward (FF)\n",
    "- Radial Basis Network (RBF)\n",
    "- Deep Feed Forward (DFF)\n",
    "- Recurrent Neural Networks (RNN)\n",
    "- Long / Short Term Memory (LSTM)\n",
    "- Gated Recurrent Unit (GRU)\n",
    "- Auto Encoder (AE)\n",
    "- Variational AE (VAE)\n",
    "- Denoising AE (DAE)\n",
    "- Sparse AE (SAE)\n",
    "- Markov Chain (MC)\n",
    "- Hopfield Network (HN)\n",
    "- Boltzmann Machine (BM)\n",
    "- Restricted BM (RBM)\n",
    "- Deep Beliefe Network (DBN)\n",
    "- Deep Convolutional Network (DCN)\n",
    "- Deconvolutional Network (DN)\n",
    "- Deep Convolutional Inverse Graphics Network (DCIGN)\n",
    "- Generative Adversarial Network (GAN)\n",
    "- Liquid State Machine (LSM)\n",
    "- Extreme Learning Machine (ELM)\n",
    "- Echo State Network (ESN)\n",
    "- Deep Residual Network (DRN)\n",
    "- Kohonen Network (KN)\n",
    "- Support Vector Machine (SVM)\n",
    "- Neural Turing Machine (NTM)\n",
    "- Convolutional Neural Network (CNN)\n",
    "\n",
    "\n",
    "Um die Grundlagen zu verstehen, haben wir uns die einzelnen Netzwerke angeschaut und und Netze gesucht, die unsere Anforderungen erfüllen.\n",
    "\n",
    "Ein Perceptron nimmt einige Eingabeparameter und addiert diese zusammen. Die Daten werden durch eine Aktivierungsfunktion geschickt und ausgegeben.\n",
    "Ein Perceptron ist ein einzelnes Neuron, somit bildet es die Grundlage für größere Netze.\n",
    "\n",
    "Bei Feed Forward Netzen sind alle Knoten vernetzt, die Daten werden von einer Schicht in die nächste Schicht übergeben. Es findet keine Wiedereinspeisung der Ergebnisse hinterer Schichten in fordere Schichten statt. Zwischen der Eingabe und der Ausgabe befindet sich ein \"hidden Layer\". Netze dieser Art werden durch \"Backpropagation\" oder auch \"Rückpropagierung\" trainiert. Dies ist ein überwachtes Lernverfahren bei dem ein externer Lehrer zu jedem Zeitpunkt einer Eingabe die gewünschte Ausgabe kennt (Vgl. [Wikipedia](https://de.wikipedia.org/wiki/Backpropagation)).\n",
    "RBFs sind FF-Netze, die statt einer logistischen Funktion eine radiale Basisfunktion verwenden (Vgl. [Wikipedia 1](https://de.wikipedia.org/wiki/Logistische_Funktion), [Wikipedia 2](https://de.wikipedia.org/wiki/Radiale_Basisfunktion)). Logistische Funktionen eignen sich vor allem bei der Beantwortung von Ja und Nein Fragen, radiale Basisfunktionen eignen sich dagegen um die Frage zu beantworten, wie weit man von seinem Ziel entfernt ist. Logistische Funktionen eignen sich daher besser für Klassifizierungen und das Treffen von Entscheidungen, als radiale Basisfunktionen.\n",
    "Deep Feed Forward Netze sind FF-Netze mit mehreren hidden Layern. Das einfache Aneinanderreihen von weiteren Schichten führt jedoch zu einem exponentiellen Wachstum von Fehlern, da mit jeder Schicht eine bestimmte Fehlerrate weitergeben wird. Es gibt jedoch Möglichkeiten diese Fehlerraten zu minimieren und DFFs effektiv nutzen zu können.\n",
    "Neben den genannten Neuronalen Netzen eignen sich Autoencoder für die Klassifizierung. Sie können zudem ohne Beaufsichtigung trainiert werden. FFs dagegen werden mit Beispieldaten verschiedener Kategorien trainiert, sodass FFs unter die Kategorie beaufsichtigtes Lernen fallen (Vgl. [towards datascience](https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464)).\n",
    "\n",
    "Aktuell werden Deep Convolutional Networks als die \"Stars\" der künstlichen Neuronalen Netze betrachtet.\n",
    "Sie besitzen verschiedene Schichten. Am anfang werden die Eingabedaten soweit komprimiert und vereinfacht, dass das Neuronale Netz die Daten verarbeiten kann.\n",
    "Das geschieht in der \"Pooling Layer\". DCNs werden typischerweise für die Erkennung verwendet.\n",
    "Bilder werden beispielsweise in kleinere Bildteile aufgeteilt. Die weiteren Schichten sind auf die Erkennung bestimmter Muster optimiert.\n",
    "Zum Beispiel kann die erste Schicht Farbverläufe erkennen, die zweite Linien, die dritte Formen, etc. bis hin zu ganzen Objekten (Vgl. [towards datascience](https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464)).\n",
    "\n",
    "CNNs sind eine Sonderform von mehrlagigen Perceptrons. Damit muss wie bei Feed Porward Netzen daraufgeachtet werden, dass die Fehlerraten nicht zu stark zunehmen.\n",
    "Zudem sind CNNs in der Lage Matrizen als Input zu verwenden. Dies können nicht alle Neuronalen Netzwerke. Häufig benötigen sie einen Vektor als Eingabeparameter (Vgl. [JAAI](https://jaai.de/convolutional-neural-networks-cnn-aufbau-funktion-und-anwendungsgebiete-1691/)).\n",
    "\n",
    "Convolutional Neural Networks sind gleichzusetzen mit Deep Convolutional Networks. \n",
    "CNNs bzw. DCNs werden zumeist verwendet um Bilder zu klassifizieren so kann ihnen ein Label gegeben werden, oder sie können in verschiedene Kategorien eingeteilt werden.\n",
    "Sie können auch verwendet werden um Objekte wie Straßenschilder oder Gesichter zu erkennen (Vgl.[Pathmind](https://wiki.pathmind.com/convolutional-network)). In der Automobilindustrie kann die Erkennung von Straßenschilder unter anderem verwendet werden, um die Geschindigkeit automatisch anzupassen, oder autonomes Fahren zu ermöglichen.\n",
    "\n",
    "Wie unsere Suche zeigt, eigenen sich für unseren Anwendungsfall der Bildklassifizierung vor allem Convolutional Neural Networks (CNNs) eignen. \n",
    "Daher setzen wir in unserem Projekt CNNs ein um unsere Bilddaten zu klassifizieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umsetzung\n",
    "Für die praktische Umsetzung, siehe `training.ipynb`.\n",
    "Für das Erkunden eines generierten Models siehe `exploration.ipynb`.\n",
    "Im Weiteren geben wir einen kleinen Überblick, wie wir unser Projekt umgesetzt haben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from src.data_loader import DataLoader as DL\n",
    "\n",
    "# Load an instance of our DataLoader\n",
    "dl = DL()\n",
    "clf_batches = MLPClassifier(hidden_layer_sizes=(128,))\n",
    "\n",
    "# Setup for batch-wise processing\n",
    "no_steps = 10000\n",
    "batch_size = 10\n",
    "\n",
    "# Store learning progress\n",
    "results = []\n",
    "\n",
    "for i in range(no_steps):\n",
    "    # Load batch of data\n",
    "    data, labels = dl.get_next_training_set(batch_size=batch_size)\n",
    "    \n",
    "    # Use partial fit to train on current batch\n",
    "    clf_batches.partial_fit(data, labels, dl.classes)\n",
    "    \n",
    "    # Test progress on random test data \n",
    "    test_x, test_y = dl.load_random_test_data(sample_size=60, return_1d=True)\n",
    "    \n",
    "    # Append result to list\n",
    "    r = clf_batches.score(test_x, test_y) +\n",
    "    results.append(1-r)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klassifizieren eigener Bilder\n",
    "Unserem Algorithmus ist es auch möglich eigens gemalte Bilder zu klassifizieren.\n",
    "In dem Notebook `exploration.ipynb` zeigen wir wir, dass mit einem vorher trainiertem Model, zufällig ausgewhlte Bilder aus dem Datensatz klassifiziert werden.\n",
    "Auch ist es möglich einen Datenpunkt aus dem Datensatz als Bild zu exportieren (siehe hierfür die Bilder in `img/test_set`.\n",
    "Als letzte Demonstration zeigen wir, wie auch handgemalte Bilder klassifiziert werden können.\n",
    "Dazu laden wir Bilder aus `img/test_hand`, welche wir selbst gemalt haben.\n",
    "Auch hier zeigen wir, dass das Model meistens in der Lage ist BIlder korrekt zu klassifiziern, auch wenn ein Helikopter gerne mal eine Pizza ist.\n",
    "\n",
    "Falls eigene Bilder klassifiziert werden sollen, können in einem geeigneten Grafikprogramm (z.B. GIMP) 256x256 Bilder gemalt werden, die dann mit der Farbtiefe `Greyscale` gespeichert werden müssen.\n",
    "Diese Bilder sind im Ordner `img/test_hand` abzulegen, aus welchen sie damm im Notebook `exploration.ipynb` geladen und klassifiziert werden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
